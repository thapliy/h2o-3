#!/usr/bin/env python
# -*- encoding: utf-8 -*-
from __future__ import unicode_literals
import bindings as bi
import sys
PY3 = sys.version_info[0] == 3
str_type = str if PY3 else (str, unicode)

# ----------------------------------------------------------------------------------------------------------------------
#   Generate per-model classes
# ----------------------------------------------------------------------------------------------------------------------

def gen_module(schema, algo, module):
    help_preamble = help_preamble_for(algo)
    help_details = help_details_for(algo)
    help_return = help_return_for(algo)
    help_epilogue = help_epilogue_for(algo)
    help_references = help_references_for(algo)
    help_example = help_example_for(algo)
    help_extra_params = help_extra_params_for(algo)
    help_extra_checks = help_extra_checks_for(algo)
    help_afterword = help_afterword_for(algo)
    model_name = algo_to_modelname(algo)

    yield "# This file is auto-generated by h2o-3/h2o-bindings/bin/gen_R.py"
    yield "# Copyright 2016 H2O.ai;  Apache License Version 2.0 (see LICENSE for details) \n#'"
    yield "# -------------------------- %s -------------------------- #" % model_name
    if help_preamble:
        lines = help_preamble.split("\n")
        for line in lines:
            yield "#' %s" % line.lstrip()

    if help_extra_params:
        lines = help_extra_params.split("\n")
        for line in lines:
            yield line.lstrip()

    for param in schema["parameters"]:
        if param["name"] in ["ignored_columns", "response_column", "max_confusion_matrix_size"]:
            continue
        if algo == "naivebayes":
            if param["name"] == "min_sdev":
                yield "#' @param threshold The minimum standard deviation to use for observations without enough data. "
                yield "#'                  Must be at least 1e-10."
                continue
            if param["name"] == "eps_sdev":
                yield "#' @param eps A threshold cutoff to deal with numeric instability, must be positive."
                continue
            if param["name"] in ["min_prob", "eps_prob"]:
                continue
        if param["name"] == "seed":
            yield "#' @param seed Seed for random numbers (affects certain parts of the algo that are stochastic and those might or might not be enabled by default)"
            if algo in ["deeplearning", "deepwater"]:
                yield "#'        Note: only reproducible when running single threaded."
            yield "#'        Defaults to -1 (time-based random number)."
            continue
        phelp = param["help"]
        if param["type"] == "boolean":
            phelp = "\code{Logical}. " + phelp
        if param["values"]:
            phelp += " Must be one of: %s." % ", ".join('"%s"' % p for p in param["values"])
        if param["default_value"] is not None:
            phelp += " Defaults to %s." % normalize_value(param, True)
        yield "#' @param %s %s" % (param["name"], bi.wrap(phelp, indent=("#'        "), indent_first=False))
    if help_details:
        yield "#' @details %s" % bi.wrap(help_details, indent=("#'          "), indent_first=False)
    if help_return:
        lines = help_return.split("\n")
        for line in lines:
            yield "%s" % line.lstrip()
        # yield "#' @return %s" % bi.wrap(help_return, indent=("#'         "), indent_first=False)
    if help_epilogue:
        yield "#' @seealso %s" % bi.wrap(help_epilogue, indent=("#'          "), indent_first=False)
    if help_references:
        yield "#' @references %s" % help_references
    if help_example:
        yield "#' @examples"
        lines = help_example.split("\n")
        for line in lines:
            yield "#' %s" % line.lstrip()
    yield "#' @export"
    yield "h2o.%s <- function(%s," % (module, get_extra_params_for(algo))
    # yield indent("training_frame,", 17 + len(algo))
    list = []
    for param in schema["parameters"]:
        if param["name"] in ["ignored_columns", "response_column", "max_confusion_matrix_size", "training_frame"]:
            continue
        if algo == "naivebayes":
            if param["name"] == "min_sdev":
                list.append(indent("threshold = %s" % normalize_value(param), 17 + len(module)))
                continue
            if param["name"] == "eps_sdev":
                list.append(indent("eps = %s" % normalize_value(param), 17 + len(module)))
                continue
            if param["name"] in ["min_prob", "eps_prob"]:
                continue
        list.append(indent("%s = %s" % (param["name"], normalize_value(param)), 17 + len(module)))
    yield ",\n".join(list)
    yield indent(") \n{", 17 + len(module))
    if algo in ["deeplearning", "deepwater", "xgboost", "drf", "gbm", "glm", "naivebayes", "stackedensemble"]:
        yield "  #If x is missing, then assume user wants to use all columns as features."
        yield "  if(missing(x)){"
        yield "     if(is.numeric(y)){"
        yield "         x <- setdiff(col(training_frame),y)"
        yield "     }else{"
        yield "         x <- setdiff(colnames(training_frame),y)"
        yield "     }"
        yield "  }"
        if algo == "gbm":
            yield "  # Required maps for different names params, including deprecated params"
            yield "  .gbm.map <- c(\"x\" = \"ignored_columns\","
            yield "                \"y\" = \"response_column\")"
        elif algo == "naivebayes":
            yield " .naivebayes.map <- c(\"x\" = \"ignored_columns\", \"y\" = \"response_column\", \n \
                         \"threshold\" = \"min_sdev\", \"eps\" = \"eps_sdev\")"
        elif algo == "glm":
            yield "  # if (!is.null(beta_constraints)) {"
            yield "  #     if (!inherits(beta_constraints, 'data.frame') && !is.H2OFrame(beta_constraints))"
            yield "  #       stop(paste('`beta_constraints` must be an H2OH2OFrame or R data.frame. Got: ', class(beta_constraints)))"
            yield "  #     if (inherits(beta_constraints, 'data.frame')) {"
            yield "  #       beta_constraints <- as.h2o(beta_constraints)"
            yield "  #     }"
            yield "  # }"
            yield "  if (inherits(beta_constraints, 'data.frame')) {"
            yield "    beta_constraints <- as.h2o(beta_constraints)"
            yield "  }"
    yield ""
    yield "  # Required args: training_frame"
    yield "  if( missing(training_frame) ) stop(\"argument \'training_frame\' is missing, with no default\")"
    # yield "  if( missing(validation_frame) ) validation_frame = NULL"
    yield "  # Training_frame must be a key or an H2OFrame object"
    yield "  if (!is.H2OFrame(training_frame))"
    yield "     tryCatch(training_frame <- h2o.getFrame(training_frame),"
    yield "           error = function(err) {"
    yield "             stop(\"argument \'training_frame\' must be a valid H2OFrame or key\")"
    yield "           })"
    if algo not in ["stackedensemble", "word2vec"]:
        yield "  # Validation_frame must be a key or an H2OFrame object"
        yield "  if (!is.null(validation_frame)) {"
        yield "     if (!is.H2OFrame(validation_frame))"
        yield "         tryCatch(validation_frame <- h2o.getFrame(validation_frame),"
        yield "             error = function(err) {"
        yield "                 stop(\"argument \'validation_frame\' must be a valid H2OFrame or key\")"
        yield "             })"
        yield "  }"
    yield "  # Parameter list to send to model builder"
    yield "  parms <- list()"
    yield "  parms$training_frame <- training_frame"
    if algo == "glrm":
        yield " if(!missing(cols))"
        yield " parms$ignored_columns <- .verify_datacols(training_frame, cols)$cols_ignore"
    elif algo in ["deeplearning", "deepwater", "xgboost", "drf", "gbm", "glm", "naivebayes", "stackedensemble"]:
        if any(param["name"] == "autoencoder" for param in schema["parameters"]):
            yield "  args <- .verify_dataxy(training_frame, x, y, autoencoder)"
        else:
            yield "  args <- .verify_dataxy(training_frame, x, y)"
        if any(param["name"] == "offset_column" for param in schema["parameters"]):
            yield "  if( !missing(offset_column) && !is.null(offset_column))  args$x_ignore <- args$x_ignore[!( offset_column == args$x_ignore )]"
        if any(param["name"] == "weights_column" for param in schema["parameters"]):
            yield "  if( !missing(weights_column) && !is.null(weights_column)) args$x_ignore <- args$x_ignore[!( weights_column == args$x_ignore )]"
        if algo != "stackedensemble":
            yield "  if( !missing(fold_column) && !is.null(fold_column)) args$x_ignore <- args$x_ignore[!( fold_column == args$x_ignore )]"
            yield "  parms$ignored_columns <- args$x_ignore"
        yield "  parms$response_column <- args$y\n"
    elif algo == "word2vec":
        yield ""
    else:
        yield "  if(!missing(x))"
        yield "    parms$ignored_columns <- .verify_datacols(training_frame, x)$cols_ignore"
    if algo == "svd":
        yield "  if(!missing(destination_key)) {"
        yield "    warning(\"'destination_key' is deprecated; please use 'model_id' instead.\")"
        yield "    if(missing(model_id)) {"
        yield "      parms$model_id <- destination_key"
        yield "    }"
        yield "  }"
    if algo == "stackedensemble":
        yield "  if (!missing(model_id))"
        yield "    parms$model_id <- model_id"
    for param in schema["parameters"]:
        if param["name"] in ["ignored_columns", "response_column", "training_frame", "max_confusion_matrix_size"]:
            continue
        if algo == "glm" and param["name"] in ["interactions", "nfolds", "beta_constraints", "missing_values_handling"]:
            continue
        if param["name"] == "loss":
            yield "  if(!missing(loss)) {"
            yield "    if(loss == \"MeanSquare\") {"
            yield "      warning(\"Loss name 'MeanSquare' is deprecated; please use 'Quadratic' instead.\")"
            yield "      parms$loss <- \"Quadratic\""
            yield "    } else "
            yield "      parms$loss <- loss"
            yield "  }"
            continue
        if param["name"] in ["min_sdev", "min_prob"]:
            yield " if (!missing(threshold))"
            yield "   parms$%s <- threshold" % param["name"]
            continue
        if param["name"] in ["eps_sdev", "eps_prob"]:
            yield " if (!missing(eps))"
            yield "   parms$%s <- eps" % param["name"]
            continue
        yield "  if (!missing(%s))" % param["name"]
        yield "    parms$%s <- %s" % (param["name"], param["name"])
    if help_extra_checks:
        lines = help_extra_checks.split("\n")
        for line in lines:
            yield "%s" % line
    if algo != "glm":
        yield "  # Error check and build model"
        yield "  .h2o.modelJob('%s', parms, h2oRestApiVersion=%d) \n}" % (algo, 99 if algo in ["svd", "stackedensemble"] else 3)
    if help_afterword:
        lines = help_afterword.split("\n")
        for line in lines:
            yield line.lstrip()

def help_preamble_for(algo):
    if algo == "deeplearning":
        return """
            Build a Deep Neural Network model using CPUs
            Builds a feed-forward multilayer artificial neural network on an H2OFrame
        """
    if algo == "stackedensemble":
        return """
            Build a stacked ensemble (aka. Super Learner) using the H2O base
            learning algorithms specified by the user.
        """
    if algo == "deepwater":
        return """
            Build a Deep Learning model using multiple native GPU backends
            Builds a deep neural network on an H2OFrame containing various data sources
        """
    if algo == "xgboost":
        return """
            Builds a eXtreme Gradient Boosting model using the native XGBoost backend
        """
    if algo == "drf":
        return """
            Builds a Random Forest Model on an H2OFrame
    """
    if algo == "gbm":
        return """
            Builds gradient boosted classification trees and gradient boosted regression trees on a parsed data set.

            The default distribution function will guess the model type based on the response column type.
            In order to run properly, the response column must be an numeric for "gaussian" or an
            enum for "bernoulli" or "multinomial".
        """
    if algo == "glm":
        return """
            Fits a generalized linear model, specified by a response variable, a set of predictors, and a
            description of the error distribution.
        """
    if algo == "glrm":
        return """
        Generalized low rank decomposition of an H2O data frame.
    """
    if algo == "kmeans":
        return """
        Performs k-means clustering on an H2O dataset.
    """
    if algo == "naivebayes":
        return """
            Compute naive Bayes probabilities on an H2O dataset.

            The naive Bayes classifier assumes independence between predictor variables conditional
            on the response, and a Gaussian distribution of numeric predictors with mean and standard
            deviation computed from the training dataset. When building a naive Bayes classifier,
            every row in the training dataset that contains at least one NA will be skipped completely.
            If the test dataset has missing values, then those predictors are omitted in the probability
            calculation during prediction.
        """
    if algo == "pca":
        return """
            Principal components analysis of an H2O data frame using the power method
            to calculate the singular value decomposition of the Gram matrix.
        """
    if algo == "svd":
        return """
        Singular value decomposition of an H2O data frame using the power method.
    """
    if algo == "word2vec":
        return """
        Trains a word2vec model on a String column of an H2O data frame.
    """

def help_details_for(algo):
    if algo == "naivebayes":
        return """The naive Bayes classifier assumes independence between predictor variables conditional
        on the response, and a Gaussian distribution of numeric predictors with mean and standard
        deviation computed from the training dataset. When building a naive Bayes classifier,
        every row in the training dataset that contains at least one NA will be skipped completely.
        If the test dataset has missing values, then those predictors are omitted in the probability
        calculation during prediction."""

def help_return_for(algo):
    if algo == "drf":
        return "#' @return Creates a \linkS4class{H2OModel} object of the right type."
    if algo == "glm":
        return """#' @return A subclass of \code{\linkS4class{H2OModel}} is returned. The specific subclass depends on the machine
        #'         learning task at hand (if it's binomial classification, then an \code{\linkS4class{H2OBinomialModel}} is
        #'         returned, if it's regression then a \code{\linkS4class{H2ORegressionModel}} is returned). The default print-
        #'         out of the models is shown, but further GLM-specifc information can be queried out of the object. To access
        #'         these various items, please refer to the seealso section below. Upon completion of the GLM, the resulting
        #'         object has coefficients, normalized coefficients, residual/null deviance, aic, and a host of model metrics
        #'         including MSE, AUC (for logistic regression), degrees of freedom, and confusion matrices. Please refer to the
        #'         more in-depth GLM documentation available here:
        #'         \\url{https://h2o-release.s3.amazonaws.com/h2o-dev/rel-shannon/2/docs-website/h2o-docs/index.html#Data+Science+Algorithms-GLM}
        """
    if algo == "kmeans":
        return "#' @return Returns an object of class \linkS4class{H2OClusteringModel}."
    if algo == "naivebayes":
        return """#' @return Returns an object of class \linkS4class{H2OBinomialModel} if the response has two categorical levels,
        #'         and \linkS4class{H2OMultinomialModel} otherwise."""
    if algo in ["glrm", "pca", "svd"]:
        return "#' @return Returns an object of class \linkS4class{H2ODimReductionModel}."

def help_epilogue_for(algo):
    if algo == "glm":
        return """\code{\link{predict.H2OModel}} for prediction, \code{\link{h2o.mse}}, \code{\link{h2o.auc}}, \code{\link{h2o.confusionMatrix}}, \code{\link{h2o.performance}}, \code{\link{h2o.giniCoef}}, \code{\link{h2o.logloss}}, \code{\link{h2o.varimp}}, \code{\link{h2o.scoreHistory}}"""
    if algo == "glrm":
        return """\code{\link{h2o.kmeans}, \link{h2o.svd}}, \code{\link{h2o.prcomp}}"""
    if algo == "kmeans":
        return """\code{\link{h2o.cluster_sizes}}, \code{\link{h2o.totss}}, \code{\link{h2o.num_iterations}}, \code{\link{h2o.betweenss}}, \code{\link{h2o.tot_withinss}}, \code{\link{h2o.withinss}}, \code{\link{h2o.centersSTD}}, \code{\link{h2o.centers}}"""
    if algo == "pca":
        return """\code{\link{h2o.svd}}, \code{\link{h2o.glrm}}"""
    if algo in ["deeplearning", "drf", "gbm"]:
        return """\code{\link{predict.H2OModel}} for prediction"""

def help_references_for(algo):
    if algo == "glrm":
        return """M. Udell, C. Horn, R. Zadeh, S. Boyd (2014). {Generalized Low Rank Models}[http://arxiv.org/abs/1410.0342]. Unpublished manuscript, Stanford Electrical Engineering Department
#'             N. Halko, P.G. Martinsson, J.A. Tropp. {Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions}[http://arxiv.org/abs/0909.4061]. SIAM Rev., Survey and Review section, Vol. 53, num. 2, pp. 217-288, June 2011."""
    if algo in ["svd", "pca"]:
        return """N. Halko, P.G. Martinsson, J.A. Tropp. {Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions}[http://arxiv.org/abs/0909.4061]. SIAM Rev., Survey and Review section, Vol. 53, num. 2, pp. 217-288, June 2011."""

def help_example_for(algo):
    if algo == "deeplearning":
        return """\donttest{
            library(h2o)
            h2o.init()
            iris.hex <- as.h2o(iris)
            iris.dl <- h2o.deeplearning(x = 1:4, y = 5, training_frame = iris.hex)

            # now make a prediction
            predictions <- h2o.predict(iris.dl, iris.hex)
            }"""
    if algo == "gbm":
        return """\donttest{
        library(h2o)
        h2o.init()

        # Run regression GBM on australia.hex data
        ausPath <- system.file("extdata", "australia.csv", package="h2o")
        australia.hex <- h2o.uploadFile(path = ausPath)
        independent <- c("premax", "salmax","minairtemp", "maxairtemp", "maxsst",
                         "maxsoilmoist", "Max_czcs")
        dependent <- "runoffnew"
        h2o.gbm(y = dependent, x = independent, training_frame = australia.hex,
                ntrees = 3, max_depth = 3, min_rows = 2)
        }"""
    if algo == "glm":
        return """\donttest{
        h2o.init()

        # Run GLM of CAPSULE ~ AGE + RACE + PSA + DCAPS
        prostatePath = system.file("extdata", "prostate.csv", package = "h2o")
        prostate.hex = h2o.importFile(path = prostatePath, destination_frame = "prostate.hex")
        h2o.glm(y = "CAPSULE", x = c("AGE","RACE","PSA","DCAPS"), training_frame = prostate.hex,
                family = "binomial", nfolds = 0, alpha = 0.5, lambda_search = FALSE)

        # Run GLM of VOL ~ CAPSULE + AGE + RACE + PSA + GLEASON
        myX = setdiff(colnames(prostate.hex), c("ID", "DPROS", "DCAPS", "VOL"))
        h2o.glm(y = "VOL", x = myX, training_frame = prostate.hex, family = "gaussian",
                nfolds = 0, alpha = 0.1, lambda_search = FALSE)


        # GLM variable importance
        # Also see:
        #   https://github.com/h2oai/h2o/blob/master/R/tests/testdir_demos/runit_demo_VI_all_algos.R
        data.hex = h2o.importFile(
          path = "https://s3.amazonaws.com/h2o-public-test-data/smalldata/demos/bank-additional-full.csv",
          destination_frame = "data.hex")
        myX = 1:20
        myY="y"
        my.glm = h2o.glm(x=myX, y=myY, training_frame=data.hex, family="binomial", standardize=TRUE,
                         lambda_search=TRUE)
        }"""
    if algo == "glrm":
        return """\donttest{
            library(h2o)
            h2o.init()
            ausPath <- system.file("extdata", "australia.csv", package="h2o")
            australia.hex <- h2o.uploadFile(path = ausPath)
            h2o.glrm(training_frame = australia.hex, k = 5, loss = "Quadratic", regularization_x = "L1",
                     gamma_x = 0.5, gamma_y = 0, max_iterations = 1000)
            }"""
    if algo == "kmeans":
        return """\donttest{
        library(h2o)
        h2o.init()
        prosPath <- system.file("extdata", "prostate.csv", package="h2o")
        prostate.hex <- h2o.uploadFile(path = prosPath)
        h2o.kmeans(training_frame = prostate.hex, k = 10, x = c("AGE", "RACE", "VOL", "GLEASON"))
        }"""
    if algo == "naivebayes":
        return """\donttest{
        h2o.init()
        votesPath <- system.file("extdata", "housevotes.csv", package="h2o")
        votes.hex <- h2o.uploadFile(path = votesPath, header = TRUE)
        h2o.naiveBayes(x = 2:17, y = 1, training_frame = votes.hex, laplace = 3)
        }"""
    if algo == "pca":
        return """\donttest{
        library(h2o)
        h2o.init()
        ausPath <- system.file("extdata", "australia.csv", package="h2o")
        australia.hex <- h2o.uploadFile(path = ausPath)
        h2o.prcomp(training_frame = australia.hex, k = 8, transform = "STANDARDIZE")
        }"""
    if algo == "svd":
        return """\donttest{
        library(h2o)
        h2o.init()
        ausPath <- system.file("extdata", "australia.csv", package="h2o")
        australia.hex <- h2o.uploadFile(path = ausPath)
        h2o.svd(training_frame = australia.hex, nv = 8)
        }"""
    if algo == "stackedensemble":
        return """
        # See example R code here: 
        # http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html
        """        

def get_extra_params_for(algo):
    if algo == "glrm":
        return "training_frame, cols = NULL"
    elif algo in ["deeplearning", "deepwater", "xgboost", "drf", "gbm", "glm", "naivebayes", "stackedensemble"]:
        return "x, y, training_frame"
    elif algo == "svd":
        return "training_frame, x, destination_key"
    elif algo == "word2vec":
        return "training_frame"
    else:
        return "training_frame, x"

def help_extra_params_for(algo):
    if algo == "glrm":
        return "#' @param cols (Optional) A vector containing the data columns on which k-means operates."
    elif algo in ["deeplearning", "deepwater", "xgboost", "drf", "gbm", "glm", "naivebayes", "stackedensemble"]:
        return """#' @param x A vector containing the names or indices of the predictor variables to use in building the model.
            #'        If x is missing,then all columns except y are used.
            #' @param y The name of the response variable in the model.If the data does not contain a header, this is the column index
            #'        number starting at 0, and increasing from left to right. (The response must be either an integer or a
            #'        categorical variable)."""
    elif algo == "svd":
        return """#' @param x A vector containing the \code{character} names of the predictors in the model.
            #' @param destination_key (Optional) The unique hex key assigned to the resulting model.
            #'                        Automatically generated if none is provided."""
    elif algo == "word2vec":
        return None
    else:
        return """#' @param x A vector containing the \code{character} names of the predictors in the model."""


def help_extra_checks_for(algo):
    if algo == "glm":
        return """
  if( !missing(interactions) ) {
    # interactions are column names => as-is
    if( is.character(interactions) )       parms$interactions <- interactions
    else if( is.numeric(interactions) )    parms$interactions <- names(training_frame)[interactions]
    else stop(\"Don't know what to do with interactions. Supply vector of indices or names\")
  }
  # For now, accept nfolds in the R interface if it is 0 or 1, since those values really mean do nothing.
  # For any other value, error out.
  # Expunge nfolds from the message sent to H2O, since H2O doesn't understand it.
  if (!missing(nfolds) && nfolds > 1)
    parms$nfolds <- nfolds
  if(!missing(beta_constraints))
    parms$beta_constraints <- beta_constraints
    if(!missing(missing_values_handling))
      parms$missing_values_handling <- missing_values_handling
  m <- .h2o.modelJob('glm', parms, h2oRestApiVersion=3)
  m@model$coefficients <- m@model$coefficients_table[,2]
  names(m@model$coefficients) <- m@model$coefficients_table[,1]
  m \n}
        """
    if algo == "glrm":
        return """
  # Check if user_y is an acceptable set of user-specified starting points
  if( is.data.frame(user_y) || is.matrix(user_y) || is.list(user_y) || is.H2OFrame(user_y) ) {
  # Convert user-specified starting points to H2OFrame
  if( is.data.frame(user_y) || is.matrix(user_y) || is.list(user_y) ) {
    if( !is.data.frame(user_y) && !is.matrix(user_y) ) user_y <- t(as.data.frame(user_y))
    user_y <- as.h2o(user_y)
  }
  parms[["user_y"]] <- user_y

  # Set k
  if( !(missing(k)) && k!=as.integer(nrow(user_y)) ) {
    warning("Argument k is not equal to the number of rows in user-specified Y. Ignoring k. Using specified Y.")
  }
  parms[["k"]] <- as.numeric(nrow(user_y))
  # } else if( is.null(user_y) ) {
  #  if(!missing(init) && parms[["init"]] == "User")
  #    warning("Initializing Y to a standard Gaussian random matrix.")
  # } else
  } else if( !is.null(user_y) )
  stop("Argument user_y must either be null or a valid user-defined starting Y matrix.")

  # Check if user_x is an acceptable set of user-specified starting points
  if( is.data.frame(user_x) || is.matrix(user_x) || is.list(user_x) || is.H2OFrame(user_x) ) {
  # Convert user-specified starting points to H2OFrame
  if( is.data.frame(user_x) || is.matrix(user_x) || is.list(user_x) ) {
    if( !is.data.frame(user_x) && !is.matrix(user_x) ) user_x <- t(as.data.frame(user_x))
    user_x <- as.h2o(user_x)
  }
  parms[["user_x"]] <- user_x
  # } else if( is.null(user_x) ) {
  #  if(!missing(init) && parms[["init"]] == "User")
  #    warning("Initializing X to a standard Gaussian random matrix.")
  # } else
  } else if( !is.null(user_x) )
  stop("Argument user_x must either be null or a valid user-defined starting X matrix.")
        """
    if algo == "kmeans":
        return """
  # Check if user_points is an acceptable set of user-specified starting points
  if( is.data.frame(user_points) || is.matrix(user_points) || is.list(user_points) || is.H2OFrame(user_points) ) {
    if ( length(init) > 1 || init == 'User') {
      parms[["init"]] <- "User"
    } else {
      warning(paste0("Parameter init must equal 'User' when user_points is set. Ignoring init = '", init, "'. Setting init = 'User'."))
    }

    parms[["init"]] <- "User"
  # Convert user-specified starting points to H2OFrame
  if( is.data.frame(user_points) || is.matrix(user_points) || is.list(user_points) ) {
    if( !is.data.frame(user_points) && !is.matrix(user_points) ) user_points <- t(as.data.frame(user_points))
    user_points <- as.h2o(user_points)
  }
  parms[["user_points"]] <- user_points
  # Set k
  if( !(missing(k)) && k!=as.integer(nrow(user_points)) ) {
    warning("Parameter k is not equal to the number of user-specified starting points. Ignoring k. Using specified starting points.")
  }
  parms[["k"]] <- as.numeric(nrow(user_points))

  } else if ( is.character(init) ) { # Furthest, Random, PlusPlus{
    parms[["user_points"]] <- NULL

  } else{
    stop ("argument init must be set to Furthest, Random, PlusPlus, or a valid set of user-defined starting points.")
  }
        """

def help_afterword_for(algo):
    if algo == "deeplearning":
        return """
            #' Anomaly Detection via H2O Deep Learning Model
            #'
            #' Detect anomalies in an H2O dataset using an H2O deep learning model with
            #' auto-encoding.
            #'
            #' @param object An \linkS4class{H2OAutoEncoderModel} object that represents the
            #'        model to be used for anomaly detection.
            #' @param data An H2OFrame object.
            #' @param per_feature Whether to return the per-feature squared reconstruction error
            #' @return Returns an H2OFrame object containing the
            #'         reconstruction MSE or the per-feature squared error.
            #' @seealso \code{\link{h2o.deeplearning}} for making an H2OAutoEncoderModel.
            #' @examples
            #' \donttest{
            #' library(h2o)
            #' h2o.init()
            #' prosPath = system.file("extdata", "prostate.csv", package = "h2o")
            #' prostate.hex = h2o.importFile(path = prosPath)
            #' prostate.dl = h2o.deeplearning(x = 3:9, training_frame = prostate.hex, autoencoder = TRUE,
            #'                                hidden = c(10, 10), epochs = 5)
            #' prostate.anon = h2o.anomaly(prostate.dl, prostate.hex)
            #' head(prostate.anon)
            #' prostate.anon.per.feature = h2o.anomaly(prostate.dl, prostate.hex, per_feature=TRUE)
            #' head(prostate.anon.per.feature)
            #' }
            #' @export
            h2o.anomaly <- function(object, data, per_feature=FALSE) {
              url <- paste0('Predictions/models/', object@model_id, '/frames/',h2o.getId(data))
              res <- .h2o.__remoteSend(url, method = "POST", reconstruction_error=TRUE, reconstruction_error_per_feature=per_feature)
              key <- res$model_metrics[[1L]]$predictions$frame_id$name
              h2o.getFrame(key)
            }

            #' Feature Generation via H2O Deep Learning Model
            #'
            #' Extract the non-linear feature from an H2O data set using an H2O deep learning
            #' model.
            #' @param object An \linkS4class{H2OModel} object that represents the deep
            #' learning model to be used for feature extraction.
            #' @param data An H2OFrame object.
            #' @param layer Index of the hidden layer to extract.
            #' @return Returns an H2OFrame object with as many features as the
            #'         number of units in the hidden layer of the specified index.
            #' @seealso \code{link{h2o.deeplearning}} for making deep learning models.
            #' @examples
            #' \donttest{
            #' library(h2o)
            #' h2o.init()
            #' prosPath = system.file("extdata", "prostate.csv", package = "h2o")
            #' prostate.hex = h2o.importFile(path = prosPath)
            #' prostate.dl = h2o.deeplearning(x = 3:9, y = 2, training_frame = prostate.hex,
            #'                                hidden = c(100, 200), epochs = 5)
            #' prostate.deepfeatures_layer1 = h2o.deepfeatures(prostate.dl, prostate.hex, layer = 1)
            #' prostate.deepfeatures_layer2 = h2o.deepfeatures(prostate.dl, prostate.hex, layer = 2)
            #' head(prostate.deepfeatures_layer1)
            #' head(prostate.deepfeatures_layer2)
            #' }
            #' @export
            h2o.deepfeatures <- function(object, data, layer = 1) {
              index = layer - 1
              url <- paste0('Predictions/models/', object@model_id, '/frames/', h2o.getId(data))
              res <- .h2o.__remoteSend(url, method = "POST", deep_features_hidden_layer=index, h2oRestApiVersion = 4)
              job_key <- res$key$name
              dest_key <- res$dest$name
              .h2o.__waitOnJob(job_key)
              h2o.getFrame(dest_key)
            }
        """
    if algo == "deepwater":
        return """
            #' Ask the H2O server whether a Deep Water model can be built (depends on availability of native backends)
            #' Returns True if a deep water model can be built, or False otherwise.
            #' @param h2oRestApiVersion (Optional) Specific version of the REST API to use
            #'
            h2o.deepwater.available <- function(h2oRestApiVersion = .h2o.__REST_API_VERSION) {
                visibility = .h2o.__remoteSend(method = "GET", h2oRestApiVersion = h2oRestApiVersion, .h2o.__MODEL_BUILDERS("deepwater"))$model_builders[["deepwater"]][["visibility"]]
                if (visibility == "Experimental") {
                    print("Cannot build a Deep Water model - no backend found.")
                    return(FALSE)
                } else {
                   return(TRUE)
                }
            }
        """
    if algo == "xgboost":
        return """
            #' Ask the H2O server whether a XGBoost model can be built (depends on availability of native backend)
            #' Returns True if a XGBoost model can be built, or False otherwise.
            #' @param h2oRestApiVersion (Optional) Specific version of the REST API to use
            #'
            h2o.xgboost.available <- function(h2oRestApiVersion = .h2o.__REST_API_VERSION) {
                visibility = .h2o.__remoteSend(method = "GET", h2oRestApiVersion = h2oRestApiVersion, .h2o.__MODEL_BUILDERS("xgboost"))$model_builders[["xgboost"]][["visibility"]]
                if (visibility == "Experimental") {
                    print("Cannot build a XGboost model - no backend found.")
                    return(FALSE)
                } else {
                   return(TRUE)
                }
            }
        """
    if algo == "glm":
        return """
            #' Set betas of an existing H2O GLM Model
            #'
            #' This function allows setting betas of an existing glm model.
            #' @param model an \linkS4class{H2OModel} corresponding from a \code{h2o.glm} call.
            #' @param beta a new set of betas (a named vector)
            #' @export
            h2o.makeGLMModel <- function(model,beta) {
               res = .h2o.__remoteSend(method="POST", .h2o.__GLMMakeModel, model=model@model_id, names = paste("[",paste(paste("\\\"",names(beta),"\\\"",sep=""), collapse=","),"]",sep=""), beta = paste("[",paste(as.vector(beta),collapse=","),"]",sep=""))
               m <- h2o.getModel(model_id=res$model_id$name)
               m@model$coefficients <- m@model$coefficients_table[,2]
               names(m@model$coefficients) <- m@model$coefficients_table[,1]
               m
            }

            #' Extract full regularization path from glm model (assuming it was run with lambda search option)
            #'
            #' @param model an \linkS4class{H2OModel} corresponding from a \code{h2o.glm} call.
            #' @export
            h2o.getGLMFullRegularizationPath <- function(model) {
               res = .h2o.__remoteSend(method="GET", .h2o.__GLMRegPath, model=model@model_id)
               colnames(res$coefficients) <- res$coefficient_names
               if(!is.null(res$coefficients_std) && length(res$coefficients_std) > 0L) {
                 colnames(res$coefficients_std) <- res$coefficient_names
               }
               res
            }

            #' Compute weighted gram matrix.
            #'
            #' @param X an \linkS4class{H2OModel} corresponding to H2O framel.
            #' @param weights character corresponding to name of weight vector in frame.
            #' @param use_all_factor_levels logical flag telling h2o whether or not to skip first level of categorical variables during one-hot encoding.
            #' @param standardize logical flag telling h2o whether or not to standardize data
            #' @param skip_missing logical flag telling h2o whether skip rows with missing data or impute them with mean
            #' @export
            h2o.computeGram <- function(X,weights="", use_all_factor_levels=FALSE,standardize=TRUE,skip_missing=FALSE) {
               res = .h2o.__remoteSend(method="GET", .h2o.__ComputeGram, X=h2o.getId(X),W=weights,use_all_factor_levels=use_all_factor_levels,standardize=standardize,skip_missing=skip_missing)
               h2o.getFrame(res$destination_frame$name)
            }

            ##' Start an H2O Generalized Linear Model Job
            ##'
            ##' Creates a background H2O GLM job.
            ##' @inheritParams h2o.glm
            ##' @return Returns a \linkS4class{H2OModelFuture} class object.
            ##' @export
            #h2o.startGLMJob <- function(x, y, training_frame, model_id, validation_frame,
            #                    #AUTOGENERATED Params
            #                    max_iterations = 50,
            #                    beta_epsilon = 0,
            #                    solver = c("IRLSM", "L_BFGS"),
            #                    standardize = TRUE,
            #                    family = c("gaussian", "binomial", "poisson", "gamma", "tweedie"),
            #                    link = c("family_default", "identity", "logit", "log", "inverse", "tweedie"),
            #                    tweedie_variance_power = NaN,
            #                    tweedie_link_power = NaN,
            #                    alpha = 0.5,
            #                    prior = 0.0,
            #                    lambda = 1e-05,
            #                    lambda_search = FALSE,
            #                    nlambdas = -1,
            #                    lambda_min_ratio = 1.0,
            #                    nfolds = 0,
            #                    beta_constraints = NULL,
            #                    ...
            #                    )
            #{
            #  # if (!is.null(beta_constraints)) {
            #  #     if (!inherits(beta_constraints, "data.frame") && !is.H2OFrame("H2OFrame"))
            #  #       stop(paste("`beta_constraints` must be an H2OH2OFrame or R data.frame. Got: ", class(beta_constraints)))
            #  #     if (inherits(beta_constraints, "data.frame")) {
            #  #       beta_constraints <- as.h2o(beta_constraints)
            #  #     }
            #  # }
            #
            #  if (!is.H2OFrame(training_frame))
            #      tryCatch(training_frame <- h2o.getFrame(training_frame),
            #               error = function(err) {
            #                 stop("argument \"training_frame\" must be a valid H2OFrame or model ID")
            #              })
            #
            #    parms <- list()
            #    args <- .verify_dataxy(training_frame, x, y)
            #    parms$ignored_columns <- args$x_ignore
            #    parms$response_column <- args$y
            #    parms$training_frame  = training_frame
            #    parms$beta_constraints = beta_constraints
            #    if(!missing(model_id))
            #      parms$model_id <- model_id
            #    if(!missing(validation_frame))
            #      parms$validation_frame <- validation_frame
            #    if(!missing(max_iterations))
            #      parms$max_iterations <- max_iterations
            #    if(!missing(beta_epsilon))
            #      parms$beta_epsilon <- beta_epsilon
            #    if(!missing(solver))
            #      parms$solver <- solver
            #    if(!missing(standardize))
            #      parms$standardize <- standardize
            #    if(!missing(family))
            #      parms$family <- family
            #    if(!missing(link))
            #      parms$link <- link
            #    if(!missing(tweedie_variance_power))
            #      parms$tweedie_variance_power <- tweedie_variance_power
            #    if(!missing(tweedie_link_power))
            #      parms$tweedie_link_power <- tweedie_link_power
            #    if(!missing(alpha))
            #      parms$alpha <- alpha
            #    if(!missing(prior))
            #      parms$prior <- prior
            #    if(!missing(lambda))
            #      parms$lambda <- lambda
            #    if(!missing(lambda_search))
            #      parms$lambda_search <- lambda_search
            #    if(!missing(nlambdas))
            #      parms$nlambdas <- nlambdas
            #    if(!missing(lambda_min_ratio))
            #      parms$lambda_min_ratio <- lambda_min_ratio
            #    if(!missing(nfolds))
            #      parms$nfolds <- nfolds
            #
            #    .h2o.startModelJob('glm', parms, h2oRestApiVersion=.h2o.__REST_API_VERSION)
            #}
        """
    if algo == "glrm":
        return """
            #' Reconstruct Training Data via H2O GLRM Model
            #'
            #' Reconstruct the training data and impute missing values from the H2O GLRM model
            #' by computing the matrix product of X and Y, and transforming back to the original
            #' feature space by minimizing each column's loss function.
            #'
            #' @param object An \linkS4class{H2ODimReductionModel} object that represents the
            #'        model to be used for reconstruction.
            #' @param data An H2OFrame object representing the training data for the H2O GLRM model.
            #'        Used to set the domain of each column in the reconstructed frame.
            #' @param reverse_transform (Optional) A logical value indicating whether to reverse the
            #'        transformation from model-building by re-scaling columns and adding back the
            #'        offset to each column of the reconstructed frame.
            #' @return Returns an H2OFrame object containing the approximate reconstruction of the
            #'         training data;
            #' @seealso \code{\link{h2o.glrm}} for making an H2ODimReductionModel.
            #' @examples
            #' \donttest{
            #' library(h2o)
            #' h2o.init()
            #' irisPath <- system.file("extdata", "iris_wheader.csv", package="h2o")
            #' iris.hex <- h2o.uploadFile(path = irisPath)
            #' iris.glrm <- h2o.glrm(training_frame = iris.hex, k = 4, transform = "STANDARDIZE",
            #'                       loss = "Quadratic", multi_loss = "Categorical", max_iterations = 1000)
            #' iris.rec <- h2o.reconstruct(iris.glrm, iris.hex, reverse_transform = TRUE)
            #' head(iris.rec)
            #' }
            #' @export
            h2o.reconstruct <- function(object, data, reverse_transform=FALSE) {
              url <- paste0('Predictions/models/', object@model_id, '/frames/',h2o.getId(data))
              res <- .h2o.__remoteSend(url, method = "POST", reconstruct_train=TRUE, reverse_transform=reverse_transform)
              key <- res$model_metrics[[1L]]$predictions$frame_id$name
              h2o.getFrame(key)
            }

            #' Convert Archetypes to Features from H2O GLRM Model
            #'
            #' Project each archetype in an H2O GLRM model into the corresponding feature
            #' space from the H2O training frame.
            #'
            #' @param object An \linkS4class{H2ODimReductionModel} object that represents the
            #'        model containing archetypes to be projected.
            #' @param data An H2OFrame object representing the training data for the H2O GLRM model.
            #' @param reverse_transform (Optional) A logical value indicating whether to reverse the
            #'        transformation from model-building by re-scaling columns and adding back the
            #'        offset to each column of the projected archetypes.
            #' @return Returns an H2OFrame object containing the projection of the archetypes
            #'         down into the original feature space, where each row is one archetype.
            #' @seealso \code{\link{h2o.glrm}} for making an H2ODimReductionModel.
            #' @examples
            #' \donttest{
            #' library(h2o)
            #' h2o.init()
            #' irisPath <- system.file("extdata", "iris_wheader.csv", package="h2o")
            #' iris.hex <- h2o.uploadFile(path = irisPath)
            #' iris.glrm <- h2o.glrm(training_frame = iris.hex, k = 4, loss = "Quadratic",
            #'                       multi_loss = "Categorical", max_iterations = 1000)
            #' iris.parch <- h2o.proj_archetypes(iris.glrm, iris.hex)
            #' head(iris.parch)
            #' }
            #' @export
            h2o.proj_archetypes <- function(object, data, reverse_transform=FALSE) {
              url <- paste0('Predictions/models/', object@model_id, '/frames/',h2o.getId(data))
              res <- .h2o.__remoteSend(url, method = "POST", project_archetypes=TRUE, reverse_transform=reverse_transform)
              key <- res$model_metrics[[1L]]$predictions$frame_id$name
              h2o.getFrame(key)
            }
        """

def algo_to_modelname(algo):
    if algo == "aggregator": return "H2O Aggregator Model"
    if algo == "deeplearning": return "Deep Learning - Neural Network"
    if algo == "deepwater": return "Deep Water - Neural Network"
    if algo == "xgboost": return "XGBoost"
    if algo == "drf": return "Random Forest Model in H2O"
    if algo == "gbm": return "Gradient Boosting Machine"
    if algo == "glm": return "H2O Generalized Linear Models"
    if algo == "glrm": return "Generalized Low Rank Model"
    if algo == "kmeans": return "KMeans Model in H2O"
    if algo == "naivebayes": return "Naive Bayes Model in H2O"
    if algo == "pca": return "Principal Components Analysis"
    if algo == "svd": return "Singular Value Decomposition"
    if algo == "stackedensemble": return "H2O Stacked Ensemble"
    return algo

def indent(string, n):
    return " " * n + string

def normalize_value(param, is_help = False):
    if not(is_help) and param["type"][:4] == "enum":  
        return "c(%s)" % ", ".join('"%s"' % p for p in param["values"])
    if param["default_value"] is None:
        if param["type"] in ["short", "int", "long", "double"]:
            return 0
        else:
            return "NULL"
    if not(is_help) and "[]" in param["type"]:
        if param["name"] == "base_models":
            return "list()"
        else:    
            return "c(%s)" % ", ".join('%s' % p for p in param["default_value"])
    if param["type"] == "boolean":
        return str(param["default_value"]).upper()
    if param["type"] == "double":
        return '%.10g' % param["default_value"]
    return param["default_value"]

# ----------------------------------------------------------------------------------------------------------------------
#   MAIN:
# ----------------------------------------------------------------------------------------------------------------------
def main():
    bi.init("R", "../../../h2o-r/h2o-package/R", clear_dir=False)

    for name, mb in bi.model_builders().items():
        if name in ["aggregator"]:
            continue
        module = name
        file_name = name
        if name == "drf":
            module = "randomForest"
            file_name = "randomforest"
        if name == "naivebayes": module = "naiveBayes"
        if name == "stackedensemble": module = "stackedEnsemble"
        if name == "pca": module = "prcomp"
        bi.vprint("Generating model: " + name)
        bi.write_to_file("%s.R" % file_name, gen_module(mb, name, module))

if __name__ == "__main__":
    main()
